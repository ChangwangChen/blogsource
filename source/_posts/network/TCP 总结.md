---
title: TCP 总结
date: 2020-08-20 13:50:45
categories: Network
tags: [network]
---

### 问题1 SYN 默认超时时间

Server端收到 Client 发送的 SYN 后会直接发送 SYN-ACK 数据包, 并将 Client 放入 SYN Queue. 如果此时 Client 下线, Server 迟迟接受不到 ACK, 那么 Server 会在一段时间内不断重新发送 SYN-ACK 数据包, 知道达到最大重试次数(tcp_synack_retries). Linux系统中, 默认重试 5 次, 重试间隔时间从 1s 开始指数增长, 5 次重试时间间隔分别为: 1s, 2s, 4s, 8s, 16s, 总共 31s, 第 5 次发出后还要在等待 32s 才知道也超时, 所以总共需要 31s + 32s = 63s 才会断开链接.

### 问题1 SYN Flood Attack: SYN 半链接洪水攻击

由于 TCP SYN Queue 的大小有限, 所以在大量的恶意客户端发送了 SYN 数据包之后直接下线, Server 端会将 SYN Queue耗尽, 让正常客户端的链接无法处理. TCP 有多种方案可以缓解这种攻击:

1. 开启 tcp_syncookies 选项
   
   当 SYN Queue 满了之后, 开启 tcp_syncookies 选项, TCP 会通过算法将 `原地址端口`, `目标地址端口` 和 `时间戳` 打造出特别的 Sequence Number返回给客户端, 如果是攻击者不会回应, 正常的客户端会在 ACK 中携带这个特殊的序列号, Server 段会解析出来客户段的信息并建立链接.
   `千万不要使用 tcp_syncookies 来处理正常的高负载链接情景`. 

2. 减少 tcp_syn_retries 次数
   
   减少此配置, 可以减少 Server 重试发送 SYN-ACK 数据包的次数, 减少超时时间.

3. 增大 tcp_max_syn_backlog (SYN Queue) 大小
   
   增大半链接队列的数目, 缓存更多的 SYN 链接. `此方法在处理 SYN Flood Attack 中不太适用`.

4. 设置 tcp_abort_on_overflow
   
   SYN Queue 满了之后直接丢弃新的 SYN 链接.

### 问题3 ISN(初始序列号) 选择

`ISN不能是固定数字`. 因为在 TCP 短时间内重连之后, 会导致 TCP 数据包的乱序. RFC793 中说, ISN 会和一个假的时钟绑定, 这个时钟每隔 4 微秒对 ISN 加 1, 直到 2^32(`TCP头部的 Sequence Number 占用 32 位`) 之后重新开始. 可以计算一个 ISN 周期是 4.55 小时. 假设 TCP 数据包的 MSL(Maximum Segment Lifetime) 小于 4.55 小时, 我们不会重用到 ISN.

### 问题4 为什么要有 TIME_WAIT 状态

TCP 的状态图中, 主动关闭的一方最终都会到达 TIME_WAIT 状态, 在等待 2MSL(`RFC793 定义 MSL 为2分钟, Linux 为30秒`) 的时间之后, 进入 CLOSED 状态正式关闭 Socket 链接. 需要 TIME_WAIT 状态的原因有2个:

1. TIME_WAIT 确保对方能够收到最后的 ACK (这里只会在此状态中收到对方重发的 FIN, 返回 ACK)
2. 确保本端之前发送的数据包全部都消失, 不会影响后续使用同一端口创建的链接.  2MSL 的时间此 socket 会被占用.

### 问题5  LAST_ACK 状态时怎么处理?

被动关闭的一方在发送 FIN 之后进入 LAST_ACK 状态, 期望收到对方返回 ACK 而进入 CLOSED 状态来关闭链接. 由于网络环境复杂, 可能出现的几种状态如下:

1. 顺利收到 ACK, 直接进入 CLOSED 而关闭链接
2. 没有收到 ACK, 超时之后重传 FIN
- 2.1 对方处于 TIME_WAIT 状态时, 对方收到 FIN 之后会再次发送 ACK, 本端收到 ACK 后进入 CLOSED 状态并关闭链接
- 2.2 对方已经进入 CLOSED 状态并关闭链接, 对方会发送 RST, 本端收到 RST 后进入 CLOSED 状态并关闭链接
3. 对方机器宕机, 此端会进入重传机制, 直到重传超时关闭链接并进入 CLOSED 状态

`总之, 被动关闭的一方在 LAST_ACK 状态一定会进入 CLOSED 状态`.

### 问题6 大量 TIME_WAIT 怎么解决?

大量的 TIME_WAIT 状态会消耗服务器的资源, 特别是端口号(端口号总数有限, 不大于 65535). 对于这种情况我们可以通过设置内核参数 `tcp_tw_reuse` 和 `tcp_timestamps` 来缓解. 使用此方法需要注意:

1. 必须同时开启这两个选项. `tcp_timestamps` 需要 socket 双方都开启.
2. `tcp_tw_recycle` 选项已经在 Linux 4.11 之后的版本中移除, 并且在开启此选项时, 在 NAT 网络环境中会造成严重问题, 所以需要谨慎开启.
3. 关于增加 `tcp_max_tw_buckets`. 此选项默认是 18000, 超过之后, 系统会直接清理掉并打印警告(`time wait bucket table overflow`).

`自我感觉还是应用程序出了问题, 需要排查`.

### 问题7 TCP重传机制

TCP 要保证所有的数据都到达并被接收, 所以要有重传机制.

接收方发送 ACK 确认只会确认最后一个连续的包, 并且 SeqNum 和 ACK 都是以字节为单位, 所以 ACK 的时候, 不能够跳着确认, 只能 ACK 接收到的最大的连续的包.

发送方重传的触发有 2 中情况:
1. 超时重传
   
   发送方会死等未确认数据包的 ACK, 当达到超时时间之后, 才会重传为确认的数据包

2. 快速重传
   
    使用数据驱动, 发送方在连续 3 次收到相同的 ACK 时候, 直接重传数据

#### 超时重传

超时重传时, 发送方只有在超过超时时间未收到数据包的 ACK, 才会重传数据. 这种情况会造成大量的等待, 产生浪费.

此机制下, 重传数据的时候会有 2 中选择:
- 只重传为收到 ACK 的数据
  
  此方法会节省宽带, 但是比较慢, 如果大量的传输失败, 则需要较长时间进行重传

- 重传 timeout 后的所有数据
  
  此方法较快, 但是可能会在成宽带的浪费


#### 快速重传

快速重传无需等待 timeout 的时间, 发送方会根据 ACK 的数据来自动发现数据丢失并进行重传. 但是重传的时候也面临和超时重传相同的问题, 不确定具体丢失数据包的序号, 只能依次重传或者全部重传.

- SACK (Selective Acknowledgment)
  
  此方式需要在接收端的 ACK 数据包头部中加入 SACK 的信息, `告诉发送方已经收到数据包的序列号`. 需要 socket 双方都打开 `tcp_sack` 选项来开启(Linux 2.4 之后默认开启).

  注意:

  1. `发送方不能将收到的 SACK 完全当作 ACK`, 即: SACK 中不包含的 SeqNumber 的数据包不能够直接删除, 因为有的时候接收方可能是因为内存等原因将接收的乱序数据包从接收缓存中删除.
  2. `SACK会消耗发送方的资源`, 恶意的返回 SACK 会导致发送方一直重复发送数据.
   
- D-SACK (Duplicate SACK) 重复收到数据
  
    `使用 SACK 来告诉发送方重复发送了哪些数据`. D-SACK 使用 SACK 的第一段来做标志:

    - 如果 SACK 的第一段的范围被 ACK 所覆盖, 那么就是 D-SACK
    - 如果 SACk 的第一段被第二段覆盖, 那么就是 D-SACK

引入了 D-SACK 有以下几点好处:

1. 发送方可以知道是数据包丢失还是 ACK 包丢失
2. 发送方会知道是不是因为 timeout 太小导致的重传
3. 网络上出现先发的数据包后达到(reordering)
4. 网络上是不是把发送的数据包复制了

### 问题8 TCP KeepAlive 机制

TCP 通过 3 个选项来维持链接的活跃, 达到一定条件的时候会关闭链接.

- tcp_keepalive_time
  
  TCP 链接空闲(IDLE)达到这个秒数的时候, 会开始发送 KeepAlive 探测包. `默认: 7200s`.

- tcp_keepalive_intvl

  TCP 发送 KeepAlive 探测包的时间间隔, 秒数. `默认: 75s`

- tcp_keepalive_probes
  
  TCP 在发送了多少个 KeepAlive 探测包都没有收到回应之后, 开始关闭链接. `默认: 9`.

所以, 在默认情况下, TCP 在经历了大约 `2小时11分钟` 的空闲加上 KeepAlive 探测之后会关闭链接.

### 问题9 RTT(Round Trip Time: 往返时间)的计算

TCP 的重传机制严重依赖 Timeout 超时时间, 并且由于 TCP 底层网络环境的动态变化, 超时时间不能设置成固定值. 所以 TCP 引入了 RTT 的概念, `一个数据包从发出到接收到 ACK 的时间间隔`. TCP 根据这个值来设置 RTO(`Retransmission TimeOut`), 提高重传机制的准确率. 计算 RTO 的几个算法介绍如下.

#### 经典算法

RFC 793 中定义如下:
1. 首先采样 RTT, 记录最近几次 RTT
2. 然后做平滑计算 SRTT(Smoothed RTT)
   
   SRTT = ( α * SRTT ) + ((1- α) * RTT)  `α 在0.8到0.9之间`
3. 计算 RTO
   
   RTO = min [ UBOUND,  max [ LBOUND,   (β * SRTT) ]  ]

   其中: 
   - UBOUND是最大的 timeout 时间, 上限值
   - LBOUND是最小的 timeout 时间, 下限值
   - β 在 1.3 到 2.0 之间

#### Karn/Partidge 算法
上面的算法会有一个问题, RTT 样本时间的选择, 会对最终 RTO 的计算产生影响(不准确导致重传机制不会高效). 计算出来的 RTO 没法准确的呈现当时的网络状态.

Karn/Partidge 算法对上述经典算法做出了改进: `忽略重传, 不把重传的 RTT 做采样`.

但是, 这样出来会有其他的问题, 比如`某个时刻开始, 网络出现抖动, 产生了比较大的延时从而导致需要重传所有的数据包(以为发生抖动之前网络正常, 所以计算出来的 RTO 会比较小, 导致数据包都会被判断超时), 又由于此算法规定重传的 RTT 不做计算, 所以 RTO 不会被更新, 导致后续的所有数据包都会被重传, 直到网络恢复正常`. 总结起来就是此算法`无法应对网络抖动导致 RTT 增加的情况`.

所以 Karn 算法使用了取巧的方式, 只要发生了重传, 直接将当前的 RTO 翻倍. 这种暴力的方式没有办法准确的呈现当前网络的状况.

#### Jacobson/Karels 算法
上述两种算法都是 `加权移动平均`, 这种方法最大的问题是如果 RTT 出现波动很难被发现. 所以有人就提出了新的算法, 引入最新的 RTT 的采样和平滑过的 SRTT 的差距做因子来计算. 公式如下:

```
SRTT = SRTT + α (RTT – SRTT)  —— 计算平滑RTT

DevRTT = (1-β)*DevRTT + β*(|RTT-SRTT|) ——计算平滑RTT和真实的差距（加权移动平均）

RTO= µ * SRTT + ∂ *DevRTT —— 神一样的公式
```

在 Linux 下, α = 0.125，β = 0.25， μ = 1，∂ = 4. (`不要问我为什么, 但它就是工作的很好`)


### 问题10 TCP 滑动窗口机制

TCP 是可靠并且有序的传输协议. 所以 TCP 需要解决可靠传输(`ACK机制保证`)和乱序问题(`重传机制保证`). 同时, TCP 需要知道网络中的数据处理带宽和处理速度的实际情况, 从而避免出现网络拥塞导致出现丢包.

TCP 引入了一些技术来网络流控, Sliding Window就是其中之一. `TCP头里有一个字段叫Window，又叫Advertised-Window，这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来`. 滑动窗口是 TCP 的被动流控, 拥塞窗口是主动流控.

滑动窗口的处理过程:
1. 三次握手的时候双方都会通知自己能够接收数据的最大字节数, 做为对方能够发送未 ACK 数据的最大值
2. 发送方发送了数据之后, 窗口中还能发送数据的大小 = 窗口大小 - 已发送数据的大小
3. 发送方收到 ACK 后, 会将滑动窗口的左边界往右移动相应的数目, 此时 ACK 中的 WIN 会决定右边界也是否向右滑动, WIN 的大小是相对左边界的大小

#### Zero Window

上面的流程中, 滑动窗口的大小可能会变成 0, 此时发送方将无法在发送数据. 为了解决这个问题, TCP 使用了 `Zero Window Probe(零窗口探测)` 技术, 发送方会发送 ZWP 包给接收方, 让接收方 ACK 自己 WIN 的尺寸. 此探测会发送 3 次, 每次大约 30 - 60 秒(具体的实现会有差异), 3 次过后 WIN 还是 0 的时候, 有的 TCP 实现会发送 RST 报文来关闭链接.

注意: TCP 中有出现等待的地方就会有可能遭受到攻击, 恶意的客户端会在建立链接之后将自己的 WIN 设置为 0, 导致服务器一直等待 ZWP, 直到资源耗尽.


#### Silly Window Syndrome(糊涂窗口综合症)

接收方忙碌, 来不及处理接收缓存中的数据, 就会导致发送方收到的 WIN 越来越小. 会出现接收方腾出及字节的 WIN, 发送方就会马上发送数据, 导致链接的利用率下降.

解决方案:
1. 问题由于接收方引起, 就会在 window size 小于某值的时候, 直接返回 0 给发送方. 直到 window 恢复到 MSS 大小或者达到接收缓存的一半, 在返回 WIN, 让发送方开始发送数据
2. 问题是发送方引起的, 就会使用 Nagle 算法. 等到 WIN >= MSS, 或者数据尺寸 >= MSS, 或者收到发送数据的 ack(时间达到 40 ms), 才会发送数据

注意: 
- TCP_NODELAY 开启 Nagle算法, 自动延时发送数据
- TCP_CORK 开启的时候, 表示手动打开延时发送, 直到`手动关闭此选项`或者`达到 200ms`的时候, 才会发送数据
- TCP_NODELAY 是发送方的限流配置, TCP_QUICKACK 是接收放的限流配置

### 问题11 Congestion Handling 拥塞控制机制

上面介绍的滑动窗口是通过双方的接收缓存尺寸来做流量控制, 无法反映出链路中的真实流量状况, TCP 作为传输层的协议需要清楚的了解链路中的所有事情.

前面介绍了 TCP 通过对 RTT 的采样来计算出 RTO, 但是因为网络的状况是在实时改变的, `如果网络上的延时突然增加, 那么, 从上面提到的 TCP 对这个事情做出的应对策略只有重传数据包, 但是, 这样会更加加重网络的负担, 接着就会出现更大的延迟出现和更多的丢包发生, 最终会进入恶性循环, 大量的 TCP 同时发生这样的状况甚至会托跨整个网络`.

因此, TCP 协议不能够忽略网络链路上面发生的事情而无脑的一直重发数据. 对此, TCP 的设计理念是: TCP 在拥塞发生的时候, 要做出自我牺牲, 主动放慢数据发送速度, 避免加重拥塞. 

TCP 的拥塞控制主要有四种算法: `慢启动`,`拥塞避免`, `拥塞发生` 和 `快速恢复`.

#### 慢热启动算法 Slow Start
   
TCP 刚刚建立链接的时候, 不要马上开启全速发送数据, 而是要一点一点提升数据的发送速度.
慢启动算法包含了 CWND (Congestion Window: 拥塞窗口), 具体的流程如下:
1. 刚建立的链接, CWND 默认是 1, 表示只能发送 1 个 MSS 大小的数据
2. 发送方没收到一个 ACK, CWND++, 呈线性增长
3. 每当过了一个 RTT, CWND*2, 呈指数增长
4. 还有一个 SSTHRESH (Slow Start Threshold), 是一个上限, 当 CWND >= SSTHRESH 时, 就会进入`拥塞避免算法`

```
注意: CWND * MSS <= WIN
发送的数据的字节数还是要受到滑动窗口大小的限制
``` 

可以看出发送速度和网络的状况有很大的关系, 如果网络很快, 那么 ACK 返回速度也很快, RTT 时间很小, 所以发包的速度很快就会达到 SSTHRESH, 慢启动算法在这种情况下一点也不慢.

```
Linux 3.0 之后, TCP 的初始 CWND 增大到了 10. 而在Linux 3.0 之前, 初始的 CWND 是根据 MSS 的值来确定的. 
例如在 Linux 2.6, 如果 MSS < 1095, 则 CWND = 4； 如果 MSS > 2190, 则 CWND = 2；其他情况 CWND = 3.
```

#### 拥塞避免算法 (Congestion Avoidance)

上面说的 CWND >= SSTHRESH 的时候, 会进入拥塞避免算法. 一般来说 SSTHRESH 的值是 65535 (字节).
算法如下:

1. 收到 ACK, CWND = CWND + 1/CWND
2. 每经过 RTT 的时间, CWND = CWND + 1

这个算法是线性增长的, 是为了避免发生网络拥塞.

#### 拥塞状态的算法

当发生丢包的时候, 会出现两种情况:

1. 等到 RTO 超时, 重传数据包.

   - SSTHRESH = cwnd / 2
   - cwnd 重置为 1
   - 进入慢启动过程

2. 连续收到 3 个重复的 ACK, 自动开启重传

    - TCP Tahoe 的实现和 RTO 一样(只是不用等到 RTO 的时间)
    - TCP Reno的实现

        - CWND = CWND / 2
        - SSTHRESH = CWND
        - 进入快速恢复算法


上面可以看出, RTO 超时之后, SSTHRESH 会变成 CWND 的一半, TCP 是在试探网络能够承载的最大流量

#### 快速恢复算法 (Fast Recovery)

1. TCP Reno

快速重传和快速恢复算法一般同时使用. 快速恢复算法认为, 能够连续接受到 3 个重复的 ACK 并且都没有达到 RTO, 说明网络状况并没有那么糟糕, 所以没必要像 RTO 那么激烈的反应. 注意: 在进入快速恢复之前, SSTHRESH 和 CWND 都已经被更改.

算法如下:

- CWND = SSTHRESH + 3 * MSS (3 表示收到了 3 个重复的 ACK)
- 重传重复 ACK 指定的数据包 (只会重传一个数据包)
- 如果在收到重复的 ACK, CWND = CWND + 1
- 如果收到了新的 ACK, 那么 CWND = SSTHRESH, 然后就进入了拥塞避免算法

此算法的问题体现在, 过于依赖 3 个重复的 ACK. 收到 3 个重复的 ACK 并不代表只丢失了一个数据包, 但是此算法只会重传一个, 并且将 CWND 进行减半. 如果大量的数据包丢失, CWND 就会指数下降, 并且不会出发 Fast Recovery 算法.

2. TCP New Reno

如果双方都实现了 SACK, 则可以清晰的知道丢失了多少包, 重传和恢复策略会更加智能. 但是对于不支持 SACK 的情况, 需要改进快速重传算法.

TCP New Reno 算法的改进如下:

- 当 Sender 收到了 3 个重复的 ACK, 进入快速重传模式, 开始重传 ACK 指示的数据包. 如果只丢失了一个数据包, 那么接下来的 ACK 会覆盖所有 Sender 已经发送数据包的 SeqNum. 如果没有覆盖, 说明不止一个包丢失.  我们把这个 ACK 称之为 Partial ACK (部分 ACK).
- 一旦 Sender 发现了 Partial ACK, Sender 就会知道多个包丢失, 于是会重传未被 ACK 覆盖的第一个数据包, 直到再也收不到 Partial ACk, 才会结束 Fast Recovery.

#### FACK 算法

Forward Acknowledgment 算法, 是基于 SACK 的算法. SACK 可以清楚的告诉 Sender 哪些包丢失, 如果丢失的包过多, 大量的重传也会加剧网络的拥塞, FACK 算法就是用来做重传过程中的拥塞流控.

- FACK 算法在发送端引入了新的变量 snd.fack, 用来保存接收端已经接收到的最大的 SeqNum
- 在非拥塞状态则和 snd.una (`snd.una 指向滑动窗口最左边的位置`) 一样处理. 
- 在拥塞状态下, snd.fack 保存着 SACK 报文 SeqNum 的最大值, 即: 接收端已经接收报文的最大 SeqNum.
- 定义 awnd = snd.nxt - snd.fack (`snd.nxt 指向了滑动窗口中即将被发送数据的位置`), 这样 awnd 就是网络上的数据. (awnd: actual quantity of data outstanding in the network)
- 如果需要重传数据, 那么, awnd = snd.nxt - snd.fack + retran_data, 即: awnd = 已经传出去的数据 + 重传的数据
- 触发 Fast Recovery 的条件是: ((snd.fack - snd.una) > (3*MSS)) || (dupacks == 3). 这样一来, 就不需要等到 3 个重复的 ACK 才会重传, 而是只要 SACK 中最大的 SeqNum 和 ACK 中的 SeqNum 比较长了 (3 个 MSS), 就会触发重传. 
- 重传的过程中, awnd 会受到 cwnd 的限制.
- 重传过程中, cwnd 不变, 直到 snd.nxt <= snd.una (`重传的数据都被确认了`), 然后会进入正常的拥塞避免机制, cwnd 开始线性上涨.

先写到这里吧.
